{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nplt.rc('font', size=16)\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2022-01-21T08:37:07.725743Z","iopub.execute_input":"2022-01-21T08:37:07.726058Z","iopub.status.idle":"2022-01-21T08:37:07.735928Z","shell.execute_reply.started":"2022-01-21T08:37:07.726018Z","shell.execute_reply":"2022-01-21T08:37:07.735115Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:07.737615Z","iopub.execute_input":"2022-01-21T08:37:07.738561Z","iopub.status.idle":"2022-01-21T08:37:07.761989Z","shell.execute_reply.started":"2022-01-21T08:37:07.738521Z","shell.execute_reply":"2022-01-21T08:37:07.760976Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"../input/training/Training.csv\")\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:07.763698Z","iopub.execute_input":"2022-01-21T08:37:07.764367Z","iopub.status.idle":"2022-01-21T08:37:07.879192Z","shell.execute_reply.started":"2022-01-21T08:37:07.764329Z","shell.execute_reply":"2022-01-21T08:37:07.878311Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"sns.set(rc={\"figure.figsize\":(12, 6)})\nsns.lineplot(data=df[:1000])","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:07.881470Z","iopub.execute_input":"2022-01-21T08:37:07.881826Z","iopub.status.idle":"2022-01-21T08:37:08.615768Z","shell.execute_reply.started":"2022-01-21T08:37:07.881785Z","shell.execute_reply":"2022-01-21T08:37:08.612291Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"From the plot we can see that there are some features that show similar trends. We can perform a deeper analysis by looking at their correlation. ","metadata":{}},{"cell_type":"code","source":"def inspect_dataframe(df, columns):\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(df[col])\n        axs[i].set_title(col)\n    plt.show()\ninspect_dataframe(df, df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:08.617222Z","iopub.execute_input":"2022-01-21T08:37:08.617703Z","iopub.status.idle":"2022-01-21T08:37:09.660666Z","shell.execute_reply.started":"2022-01-21T08:37:08.617665Z","shell.execute_reply":"2022-01-21T08:37:09.660015Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:09.662067Z","iopub.execute_input":"2022-01-21T08:37:09.663000Z","iopub.status.idle":"2022-01-21T08:37:09.720661Z","shell.execute_reply.started":"2022-01-21T08:37:09.662962Z","shell.execute_reply":"2022-01-21T08:37:09.719854Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"df_std = df\ndf_std = df_std.melt(var_name='Column', value_name='Not Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Not Normalized', data=df_std)\n_ = ax.set_xticklabels(df.columns, rotation=90)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:09.722193Z","iopub.execute_input":"2022-01-21T08:37:09.723223Z","iopub.status.idle":"2022-01-21T08:37:11.712091Z","shell.execute_reply.started":"2022-01-21T08:37:09.723184Z","shell.execute_reply":"2022-01-21T08:37:11.711226Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"We see that the dataset is not normalized. ","metadata":{}},{"cell_type":"markdown","source":"# Correlation Analysis","metadata":{}},{"cell_type":"code","source":"from scipy.stats import pearsonr\n\ndef corrfunc(x,y, ax=None, **kws):\n    \"\"\"Plot the correlation coefficient in the top left hand corner of a plot.\"\"\"\n    r, _ = pearsonr(x, y)\n    ax = ax or plt.gca()\n    # Unicode for lowercase rho (œÅ)\n    rho = '\\u03C1'\n    ax.annotate(f'{rho} = {r:.2f}', xy=(.1, .9), xycoords=ax.transAxes)\n    \ndef corrdot(*args, **kwargs):\n    corr_r = args[0].corr(args[1], 'pearson')\n    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n    ax = plt.gca()\n    ax.set_axis_off()\n    marker_size = abs(corr_r) * 10000\n    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap=\"Blues\",\n               vmin=-1, vmax=1, transform=ax.transAxes)\n    font_size = abs(corr_r) * 40 + 5\n    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\",\n                ha='center', va='center', fontsize=font_size)    \n    \n# g = sns.pairplot(stocks,palette=[\"Blues_d\"])\ng = sns.PairGrid(df, aspect=1.4, diag_sharey=False)\ng.map_lower(corrfunc)\ng.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'Black','linewidth':1})\ng.map_diag(sns.distplot, kde_kws={'color': 'Black','linewidth':1})\ng.map_upper(corrdot)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:11.715940Z","iopub.execute_input":"2022-01-21T08:37:11.716458Z","iopub.status.idle":"2022-01-21T08:37:46.194852Z","shell.execute_reply.started":"2022-01-21T08:37:11.716415Z","shell.execute_reply":"2022-01-21T08:37:46.194094Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"'Crunchiness' and 'Hype root' have an high correlation coefficient. This means that they are strongly correlated, in the sense that if one of them will increase, also the other will be prone to increase. Same reasonment holds for 'Wonder Level' and 'Loudiness on impact'.","metadata":{}},{"cell_type":"code","source":"#drop crunchiness (hype root remains) and Wonder level (loudiness on impact remains)\ndf= df.drop(['Crunchiness', 'Wonder level'], axis=1)\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:46.196609Z","iopub.execute_input":"2022-01-21T08:37:46.197184Z","iopub.status.idle":"2022-01-21T08:37:46.211793Z","shell.execute_reply.started":"2022-01-21T08:37:46.197136Z","shell.execute_reply":"2022-01-21T08:37:46.211112Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#normalization\nscaler = MinMaxScaler()\nndf = pd.DataFrame(scaler.fit_transform(df),columns=df.columns)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:46.215596Z","iopub.execute_input":"2022-01-21T08:37:46.216199Z","iopub.status.idle":"2022-01-21T08:37:46.228074Z","shell.execute_reply.started":"2022-01-21T08:37:46.216144Z","shell.execute_reply":"2022-01-21T08:37:46.227326Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"ndf.describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:46.229540Z","iopub.execute_input":"2022-01-21T08:37:46.230145Z","iopub.status.idle":"2022-01-21T08:37:46.277775Z","shell.execute_reply.started":"2022-01-21T08:37:46.230105Z","shell.execute_reply":"2022-01-21T08:37:46.277068Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"df_std = (ndf - ndf.mean()) / ndf.std()\ndf_std = df_std.melt(var_name='Column', value_name='Normalized')\nplt.figure(figsize=(12, 6))\nax = sns.violinplot(x='Column', y='Normalized', data=df_std)\n_ = ax.set_xticklabels(ndf.columns, rotation=90)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:46.279323Z","iopub.execute_input":"2022-01-21T08:37:46.279898Z","iopub.status.idle":"2022-01-21T08:37:47.721598Z","shell.execute_reply.started":"2022-01-21T08:37:46.279862Z","shell.execute_reply":"2022-01-21T08:37:47.718940Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"test_size = 3500 #dei 68528 campioni che ho, considero gli utlimi 3500 come test\nX_train_raw = ndf.iloc[:-test_size]\nX_test_raw = ndf.iloc[-test_size:]\nprint(X_train_raw.shape, X_test_raw.shape)\n\n\nfor col,cont in ndf.iteritems():\n    plt.figure(figsize=(17,5))\n    plt.plot(cont, label='Train {}'.format(col))\n    plt.plot(X_test_raw[col], label='Test {}'.format(col))\n    plt.title('Train-Test Split')\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:47.723223Z","iopub.execute_input":"2022-01-21T08:37:47.723792Z","iopub.status.idle":"2022-01-21T08:37:49.533717Z","shell.execute_reply.started":"2022-01-21T08:37:47.723756Z","shell.execute_reply":"2022-01-21T08:37:49.533008Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"print(X_train_raw.shape,X_test_raw.shape)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:49.535310Z","iopub.execute_input":"2022-01-21T08:37:49.535880Z","iopub.status.idle":"2022-01-21T08:37:49.541021Z","shell.execute_reply.started":"2022-01-21T08:37:49.535844Z","shell.execute_reply":"2022-01-21T08:37:49.540301Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def build_sequences(df, target_labels, window, stride, telescope):\n  #the telescope tells us how many parameters i want to predict in the future\n    # Sanity check to avoid runtime errors\n    assert window % stride == 0\n    dataset = []\n    labels = []\n    temp_df = df.copy().values\n    #target_labels allows to choose how many sensors we are going to predict in the future\n    temp_label = df[target_labels].copy().values\n    padding_len = len(df)%window\n\n    if(padding_len != 0):\n        # Compute padding length\n        padding_len = window - len(df)%window\n        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float64')\n        temp_df = np.concatenate((padding,df))\n        padding = np.zeros((padding_len,temp_label.shape[1]), dtype='float64')\n        temp_label = np.concatenate((padding,temp_label))\n        assert len(temp_df) % window == 0\n\n    for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n        dataset.append(temp_df[idx:idx+window])\n        labels.append(temp_label[idx+window:idx+window+telescope])\n\n    dataset = np.array(dataset)\n    labels = np.array(labels)\n    return dataset, labels","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:49.542792Z","iopub.execute_input":"2022-01-21T08:37:49.543480Z","iopub.status.idle":"2022-01-21T08:37:49.555908Z","shell.execute_reply.started":"2022-01-21T08:37:49.543445Z","shell.execute_reply":"2022-01-21T08:37:49.555012Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# Training: one step prediction","metadata":{}},{"cell_type":"code","source":"# training dataset parameters\nwindow=300 # size of the set of samples we're using to train the NN\nstride=10 # space between the beginning of one window and the next\ntelescope=864 # How many steps to predict in the future\ntarget_labels = ndf.columns","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:49.557631Z","iopub.execute_input":"2022-01-21T08:37:49.558407Z","iopub.status.idle":"2022-01-21T08:37:49.564464Z","shell.execute_reply.started":"2022-01-21T08:37:49.558368Z","shell.execute_reply":"2022-01-21T08:37:49.563627Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"X_min = X_train_raw.min()\nX_max = X_train_raw.max()\nfuture = df[-window:]\nfuture = (future-X_min)/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0)\nfuture.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:49.566245Z","iopub.execute_input":"2022-01-21T08:37:49.567090Z","iopub.status.idle":"2022-01-21T08:37:49.581668Z","shell.execute_reply.started":"2022-01-21T08:37:49.567050Z","shell.execute_reply":"2022-01-21T08:37:49.580994Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_test, y_test = build_sequences(X_test_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:49.582930Z","iopub.execute_input":"2022-01-21T08:37:49.583476Z","iopub.status.idle":"2022-01-21T08:37:49.723455Z","shell.execute_reply.started":"2022-01-21T08:37:49.583440Z","shell.execute_reply":"2022-01-21T08:37:49.722603Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"def inspect_multivariate(X, y, columns, telescope, idx=None):\n    if(idx==None):\n        idx=np.random.randint(0,len(X))\n\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n        axs[i].scatter(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), y[idx,:,i], color='orange')\n        axs[i].set_title(col)\n        axs[i].set_ylim(0,1)\n        axs[i].legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:49.724997Z","iopub.execute_input":"2022-01-21T08:37:49.725288Z","iopub.status.idle":"2022-01-21T08:37:49.735428Z","shell.execute_reply.started":"2022-01-21T08:37:49.725251Z","shell.execute_reply":"2022-01-21T08:37:49.734482Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"inspect_multivariate(X_train, y_train, target_labels, telescope)","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:49.737169Z","iopub.execute_input":"2022-01-21T08:37:49.737417Z","iopub.status.idle":"2022-01-21T08:37:50.831585Z","shell.execute_reply.started":"2022-01-21T08:37:49.737384Z","shell.execute_reply":"2022-01-21T08:37:50.827367Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# model training parameters\ninput_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 32\nepochs = 200","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:50.833073Z","iopub.execute_input":"2022-01-21T08:37:50.833683Z","iopub.status.idle":"2022-01-21T08:37:50.838384Z","shell.execute_reply.started":"2022-01-21T08:37:50.833647Z","shell.execute_reply":"2022-01-21T08:37:50.837499Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"# Models","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import GRU, LSTM, Dense, Reshape, Conv1D, MaxPool1D","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:50.839949Z","iopub.execute_input":"2022-01-21T08:37:50.840639Z","iopub.status.idle":"2022-01-21T08:37:50.847818Z","shell.execute_reply.started":"2022-01-21T08:37:50.840601Z","shell.execute_reply":"2022-01-21T08:37:50.846874Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"models = {}\nresults = {}","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:50.849629Z","iopub.execute_input":"2022-01-21T08:37:50.850354Z","iopub.status.idle":"2022-01-21T08:37:50.856851Z","shell.execute_reply.started":"2022-01-21T08:37:50.850255Z","shell.execute_reply":"2022-01-21T08:37:50.856084Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"models['modeldropped'] = Sequential([\n    GRU(256, input_shape = input_shape, return_sequences=False,name=\"gru1\"),\n    \n    Dense(output_shape[-1]*output_shape[-2], activation='relu', name=\"dense\"),\n    Reshape((output_shape[-2],output_shape[-1]), name=\"reshape_to_batch\"),\n    Conv1D(output_shape[-1], 1, padding='same', name=\"finalconv\"),\n])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:50.858546Z","iopub.execute_input":"2022-01-21T08:37:50.859362Z","iopub.status.idle":"2022-01-21T08:37:51.083843Z","shell.execute_reply.started":"2022-01-21T08:37:50.859325Z","shell.execute_reply":"2022-01-21T08:37:51.083140Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"for model in models.values():\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.RMSprop(), metrics=['mae'])\n    model.summary()\n    print('***\\n\\n')","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:51.085731Z","iopub.execute_input":"2022-01-21T08:37:51.085989Z","iopub.status.idle":"2022-01-21T08:37:51.103724Z","shell.execute_reply.started":"2022-01-21T08:37:51.085953Z","shell.execute_reply":"2022-01-21T08:37:51.102964Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"for key, model in models.items():\n    results[key] = model.fit(\n        x = X_train,\n        y = y_train,\n        batch_size = batch_size,\n        epochs = epochs,\n        validation_split=.2,\n        callbacks = [\n           tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=20, restore_best_weights=True),\n           tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=5, factor=0.5, min_lr=1e-4)\n        ]\n    ).history","metadata":{"execution":{"iopub.status.busy":"2022-01-21T08:37:51.105004Z","iopub.execute_input":"2022-01-21T08:37:51.105791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for key, model in models.items():\n    model.save(f'models/{key}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 1, figsize=(16, 24))\n\ncolors = ['red', 'green', 'blue', 'orange']\n\nfor index, (key, history) in enumerate(results.items()):\n    best_epoch = np.argmin(history['val_loss'])\n\n    ax[0].plot(history['loss'], label=f'{key} Train. loss', alpha=.9, color=colors[index])\n    ax[0].plot(history['val_loss'], label=f'{key} Valid. loss', alpha=.9, color=colors[index], ls='--')\n    ax[0].axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='-.', color=colors[index])\n    #ax[0].axis([0, 80, 0, 0.02])\n    ax[0].set_title('Mean Squared Error (Loss)')\n    ax[0].legend()\n    ax[0].grid(alpha=.3)\n\n    ax[1].plot(history['mae'], label=f'{key} Train. acc.', alpha=.9, color=colors[index])\n    ax[1].plot(history['val_mae'], label=f'{key} Valid. acc.', alpha=.9, color=colors[index], ls='--')\n    ax[1].axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='-.', color=colors[index])\n    #ax[1].axis([0, 80, 0, 0.10])\n    ax[1].set_title('Mean Absolute Error')\n    ax[1].legend()\n    ax[1].grid(alpha=.3)\n\n    ax[2].plot(history['lr'], label='Second model Learning Rate', alpha=.8, color=colors[index])\n    ax[2].axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='-.', color=colors[index])\n    ax[2].legend()\n    ax[2].grid(alpha=.3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}